import os
import uuid
import tempfile
from typing_extensions import TypedDict, List
import streamlit as st
import openai
import faiss
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langgraph.graph import START, StateGraph
from utils import get_context_length_limit, update_api_and_model, authenticate, stream_enabled, select_embedding_model


@st.cache_resource(max_entries=1)
def get_vector_store(uploaded_pdf):

    # Read documents
    docs = []
    temp_dir = tempfile.TemporaryDirectory()
    temp_filepath = os.path.join(temp_dir.name, uploaded_pdf.name)
    with open(temp_filepath, "wb") as f:
        f.write(uploaded_pdf.getvalue())
    loader = PyPDFLoader(temp_filepath)
    docs.extend(loader.load())

    # Split documents
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", " ", ""],
        chunk_size=800,
        chunk_overlap=50,
        length_function=len,
    )
    all_splits = text_splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings(
        model=st.session_state["embedding_model"], 
        openai_api_key=st.session_state["api_key"]
    )

    index = faiss.IndexFlatL2(len(embeddings.embed_query("hello world")))

    vector_store = FAISS(
        embedding_function=embeddings,
        index=index,
        docstore=InMemoryDocstore(),
        index_to_docstore_id={},
    )
    
    uuids = [str(uuid.uuid4()) for _ in range(len(all_splits))]

    vector_store.add_documents(documents=all_splits, ids=uuids)

    return vector_store

@st.cache_resource(max_entries=1)
def app(model):

    Chat_Model = ChatOpenAI if st.session_state.api == "OpenAI" else ChatGroq
    llm = Chat_Model(model=model, api_key=st.session_state.api_key)

    template = """
    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.

    Question: {question}

    Context: {context}

    Answer:"""

    prompt_template = PromptTemplate.from_template(template)    

    class State(TypedDict):
        question: str
        context: List[Document]
        answer: str

    graph_builder = StateGraph(state_schema=State)

    def retrieve(state: State):
        retrieved_docs = vector_store.similarity_search(state["question"], k=2)
        return {"context": retrieved_docs}

    def generate(state: State):
        docs_content = "\n\n".join(doc.page_content for doc in state["context"])
        messages = prompt_template.invoke({"question": state["question"], "context": docs_content})
        response = llm.invoke(messages)
        return {"answer": response.content}
    
    graph_builder.add_node("retrieve", retrieve)
    graph_builder.add_node("generate", generate)

    graph_builder.add_edge(START, "retrieve")
    graph_builder.add_edge("retrieve", "generate")

    app = graph_builder.compile()

    return app

def stream_wrapper(stream):
    for chunk, _ in stream:
        yield chunk.content

# UI Setup
st.header(":material/docs: RAG")

# Sidebar
model = update_api_and_model("OpenAI")
select_embedding_model()
authenticate()
context_length_limit = get_context_length_limit()
is_stream = stream_enabled()

uploaded_pdf = st.file_uploader(
    label='Choose your .pdf file',
    type="pdf",
    accept_multiple_files =False,
    disabled=not st.session_state["valid_auth"]
)

if not uploaded_pdf:
    st.info("Please upload a PDF document to continue.")

continue_run = st.session_state["valid_auth"] and uploaded_pdf

if continue_run:
    vector_store = get_vector_store(uploaded_pdf)
    graph = app(model)
    
if question := st.chat_input(disabled=not continue_run):
    st.chat_message(name="human").write(question)

    if not is_stream:
        response = graph.invoke({"question": question})
        st.chat_message(name="assistant").write(response["answer"])
    else:
        stream = graph.stream({"question": question}, stream_mode="messages")
        st.chat_message(name="assistant").write(stream_wrapper(stream))
